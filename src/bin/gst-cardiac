#!/usr/bin/env python


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


import logging
from optparse import OptionParser
import os
import sys


from gi.repository import GObject
GObject.threads_init()
from gi.repository import Gst
Gst.init(None)


__author__ = "Kipp Cannon <kcannon@cita.utoronto.ca>"
__version__ = "FIXME"
__date__ = "FIXME"


#
# =============================================================================
#
#                            Pipeline Construction
#
# =============================================================================
#


def mkelem(pipeline, src, elem_type_name, **properties):
	elem = Gst.ElementFactory.make(elem_type_name, properties.pop("name", None))
	if elem is None:
		raise ValueError("failed to create element '%s'" % elem_type_name)
	for name, value in properties.items():
		elem.set_property(name.replace("_", "-"), value)
	pipeline.add(elem)
	if isinstance(src, Gst.Pad):
		src.get_parent_element().link_pads(src, elem, None)
	elif src is not None:
		src.link(elem)
	return elem


def write_dump_dot(pipeline, filestem, verbose = False):
	"""
	This function needs the environment variable GST_DEBUG_DUMP_DOT_DIR
	to be set.   The filename will be

	os.path.join($GST_DEBUG_DUMP_DOT_DIR, filestem + ".dot")

	If verbose is True, a message will be written to stderr.
	"""
	if "GST_DEBUG_DUMP_DOT_DIR" not in os.environ:
		raise ValueError("cannot write pipeline, environment variable GST_DEBUG_DUMP_DOT_DIR is not set")
	Gst.debug_bin_to_dot_file(pipeline, Gst.DebugGraphDetails.ALL, filestem)
	if verbose:
		print >>sys.stderr, "Wrote pipeline to %s" % os.path.join(os.environ["GST_DEBUG_DUMP_DOT_DIR"], "%s.dot" % filestem)


class src_deferred_link(object):
	def __init__(self, src, srcpadname, sinkpad):
		no_more_pads_handler_id = src.connect("no-more-pads", self.no_more_pads, srcpadname)
		src.connect("pad-added", self.pad_added, (srcpadname, sinkpad, no_more_pads_handler_id))

	@staticmethod
	def pad_added(element, pad, (srcpadname, sinkpad, no_more_pads_handler_id)):
		if pad.get_name() == srcpadname:
			pad.get_parent().handler_disconnect(no_more_pads_handler_id)
			pad.link(sinkpad)

	@staticmethod
	def no_more_pads(element, srcpadname):
		#write_dump_dot(element.get_parent(), "pipeline")
		raise ValueError("%s: no pad named '%s'" % (element.get_name(), srcpadname))


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version = "%prog ??",
		usage = "%prog [options] filename",
		description = "Extract cardiac pulse from a video of a patient."
	)
	parser.add_option("--input", metavar = "filename", default = "/dev/video0", help = "Set the name of the input file (default = \"/dev/video0\").  If the filename starts with \"/dev/\" then it is assumed to be a V4L device otherwise it is assumed to be a video file.")
	parser.add_option("--brightness", metavar = "[-1, +1]", type = "float", help = "Adjust brightness for face detection (skin colour is computed from original video).")
	parser.add_option("--contrast", metavar = "[0, 2]", type = "float", help = "Adjust contrast for face detection (skin colour is computed from original video).")
	parser.add_option("--gamma", metavar = "gamma", type = "float", default = 1.6, help = "Set gamma correction (default = 1.6).")
	parser.add_option("--no-display", action = "store_true", help = "Do not display video in window (allows code to run faster than realtime).")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")

	options, filenames = parser.parse_args()

	if options.verbose:
		logging.basicConfig(level = logging.INFO)

	return options, filenames


#
# =============================================================================
#
#                                   Pipeline
#
# =============================================================================
#


class Handler(object):
	# FIXME:  remove when gstreamer wrapping can retrieve message
	# object properly
	import re
	facesparser = re.compile(r'.*faces=[^{]*\{ *(?:"([^"]*)")+ *\}.*')
	faceparser = re.compile(r'(?P<name>[^ =]*)=\([^)]*\)(?P<value>[^ ,;]*)')

	def __init__(self, mainloop, pipeline, gamma):
		self.mainloop = mainloop
		self.pipeline = pipeline
		self.gamma = gamma

		self.video_src = None
		self.face_processors = []

		bus = pipeline.get_bus()
		bus.add_signal_watch()
		bus.connect("message", self.on_message)

	def on_message(self, bus, message):
		if message.type == Gst.MessageType.ELEMENT:
			s = message.get_structure()
			if s.get_name() == "facedetect":
				self.do_facedetect_message(message.src, s)
		elif message.type == Gst.MessageType.EOS:
			self.pipeline.set_state(Gst.State.NULL)
			self.mainloop.quit()
		elif message.type == Gst.MessageType.ERROR:
			gerr, dbgmsg = message.parse_error()
			logging.fatal("(%s:%d '%s'): %s" % (gerr.domain, gerr.code, gerr.message, dbgmsg))
			sys.exit(1)

	def do_facedetect_message(self, elem, s):
		#
		# s is a GstStructure w/ elements:
		#
		#   timestamp:  uint64
		#   duration:  uint64
		#	timestamp and duration of current video buffer
		#   stream-time:  uint64
		#   running-time:  uint64
		#	of location in stream
		#   faces:  list
		#	containing structures with name "face" and elements:
		#	   x
		#	   y
		#	   width
		#	   height
		#		top-left corner and extent of face
		#	   nose->x
		#	   nose->y
		#	   nose->width
		#	   nose->height
		#		top-left corner and extent of nose
		#	   mouth->x
		#	   mouth->y
		#	   mouth->width
		#	   mouth->height
		#		top-left corner and extent of mouth
		#	   eyes->x
		#	   eyes->y
		#	   eyes->width
		#	   eyes->height
		#		top-left corner and extent of eyes
		#

		# FIXME:  return to .get_value() when gstreamer wrapping
		# can handle messages properly
		#faces = s.get_value("faces")
		faces = [dict((name, int(value)) for name, value in self.faceparser.findall(face)) for face in self.facesparser.findall(s.to_string().replace("\\", ""))]

		#
		# disallow more than one face
		#

		if len(faces) > 1:
			return

		#
		# do we need to add face processors?
		#

		if len(faces) > len(self.face_processors):
			self.pipeline.set_state(Gst.State.PAUSED)
			while len(faces) > len(self.face_processors) and self.video_src is not None:
				queue = mkelem(self.pipeline, None, "queue", max_size_time = Gst.SECOND)
				queue.set_state(Gst.State.PAUSED)
				self.face_processors.append(mkelem(self.pipeline, queue, "faceprocessor"))
				Gst.ChildProxy.set_property(self.face_processors[-1], "face2rgb::gamma", self.gamma)
				self.face_processors[-1].set_state(Gst.State.PAUSED)
				self.video_src.link(queue)
			self.pipeline.set_state(Gst.State.PLAYING)

		#
		# do we need to remove any?
		#

		if len(faces) < len(self.face_processors):
			pass
			#for face_processor in self.face_processors[len(faces):]:
			#	self.pipeline.remove(face_processor)

		#
		# update face locations
		#

		for faceprocessor, face in zip(self.face_processors, faces):
			Gst.ChildProxy.set_property(faceprocessor, "face2rgb::face-x", face["x"])
			Gst.ChildProxy.set_property(faceprocessor, "face2rgb::face-y", face["y"])
			Gst.ChildProxy.set_property(faceprocessor, "face2rgb::face-width", face["width"])
			Gst.ChildProxy.set_property(faceprocessor, "face2rgb::face-height", face["height"])
			if "nose->x" in face:
				Gst.ChildProxy.set_property(faceprocessor, "face2rgb::nose-x", face["nose->x"])
				Gst.ChildProxy.set_property(faceprocessor, "face2rgb::nose-y", face["nose->y"])
				Gst.ChildProxy.set_property(faceprocessor, "face2rgb::nose-width", face["nose->width"])
				Gst.ChildProxy.set_property(faceprocessor, "face2rgb::nose-height", face["nose->height"])
		#write_dump_dot(self.pipeline, "blah", verbose = True)


options, filenames = parse_command_line()


pipeline = Gst.Pipeline()
mainloop = GObject.MainLoop()
handler = Handler(mainloop, pipeline, options.gamma)

#
# get video stream
#

if options.input.startswith("/dev/"):
	src = mkelem(pipeline, mkelem(pipeline, None, "v4l2src", device = options.input), "videoconvert")
else:
	src = mkelem(pipeline, mkelem(pipeline, None, "filesrc", location = options.input), "decodebin")
	elem = mkelem(pipeline, None, "videoconvert")
	src_deferred_link(src, "src_0", elem.get_static_pad("sink"))
	src = elem

handler.video_src = src = mkelem(pipeline, mkelem(pipeline, src, "capsfilter", caps = Gst.Caps.from_string("video/x-raw, format=(string)RGB")), "tee")

#
# limit frame rate into face detector to 5 frames per second (don't need to
# recompute face mask faster than this)
#

src = mkelem(pipeline, mkelem(pipeline, src, "videorate", drop_only = True), "capsfilter", caps = Gst.Caps.from_string("video/x-raw, framerate=5/1"))

#
# adjust brightness and contrast before face detection if requested
#

if options.brightness is not None or options.contrast is not None:
	kwargs = {}
	if options.brightness is not None:
		kwargs["brightness"] = options.brightness
	if options.contrast is not None:
		kwargs["contrast"] = options.contrast
	src = mkelem(pipeline, src, "videobalance", **kwargs)

#
# do face detection.  auto-adjust minimum face size to fixed fraction of
# video frame (default is 1x1 which is very CPU-intensive)
#

def facedetect_sink_caps_hander(pad, pspec, ignored):
	caps = pad.get_property("caps")
	if caps is not None:
		s = caps.get_structure(0)
		success, width = s.get_int("width")
		success, height = s.get_int("height")
		min_width = max(width // 8, 1)
		min_height = max(height // 6, 1)
		logging.info("video is %dx%d pixels, setting minimum face size to %dx%d pixels" % (width, height, min_width, min_height))
		elem = pad.get_parent()
		elem.set_property("min-size-width", min_width)
		elem.set_property("min-size-height", min_height)
src = mkelem(pipeline, src, "facedetect", updates = 1, scale_factor = 1.1, display = not options.no_display)
src.get_static_pad("sink").connect("notify::caps", facedetect_sink_caps_hander, None)

#
# display video, or not
#

if options.no_display:
	mkelem(pipeline, src, "fakesink", sync = False, async = False)
else:
	src = mkelem(pipeline, src, "queue", max_size_buffers = 0, max_size_time = 2 * Gst.SECOND, max_size_bytes = 0)
	mkelem(pipeline, mkelem(pipeline, src, "videoconvert"), "autovideosink")

#
# run pipeline
#

pipeline.set_state(Gst.State.PLAYING)
mainloop.run()
